{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RkanGen/LLM-RAG/blob/main/Building_a_Multi_Document_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b523e0a",
      "metadata": {
        "id": "0b523e0a"
      },
      "source": [
        "# Building a Multi-Document Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a323703",
      "metadata": {
        "id": "1a323703"
      },
      "source": [
        "## 1-Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f"
      },
      "outputs": [],
      "source": [
        "from helper import get_openai_api_key\n",
        "OPENAI_API_KEY = get_openai_api_key()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3221a474-5817-4db2-af46-e029042a75a5",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "3221a474-5817-4db2-af46-e029042a75a5"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20adaa26",
      "metadata": {
        "id": "20adaa26"
      },
      "source": [
        "## 1. Setup an agent over 3 papers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48b71ff6",
      "metadata": {
        "id": "48b71ff6"
      },
      "source": [
        "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
      "metadata": {
        "height": 200,
        "tags": [],
        "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
      "metadata": {
        "height": 166,
        "tags": [],
        "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
        "outputId": "c98d9bb5-a0df-4f6e-c0f6-686ebe2d536e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n",
            "Getting tools for paper: selfrag.pdf\n"
          ]
        }
      ],
      "source": [
        "from utils import get_doc_tools\n",
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07"
      },
      "outputs": [],
      "source": [
        "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff58c52",
      "metadata": {
        "height": 64,
        "id": "bff58c52"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2c6a9f",
      "metadata": {
        "height": 30,
        "id": "2f2c6a9f",
        "outputId": "1f381188-c3d1-4687-cb7a-81eae1d84055"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(initial_tools)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a124a438-5609-402e-8642-69d1088cb9ad",
      "metadata": {
        "height": 183,
        "tags": [],
        "id": "a124a438-5609-402e-8642-69d1088cb9ad"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    initial_tools,\n",
        "    llm=llm,\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
        "outputId": "35fad8b4-29a2-424c-96fe-a6c479fcef9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
            "=== Function Output ===\n",
            "PG19 test split\n",
            "=== Calling Function ===\n",
            "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
            "=== Function Output ===\n",
            "The evaluation results show that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. Additionally, the models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results even on extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
            "=== LLM Response ===\n",
            "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
            "\n",
            "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. The models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results even on extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
        "    \"and then tell me about the evaluation results\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ace340b1-761f-4058-be41-68cf131541e4",
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "ace340b1-761f-4058-be41-68cf131541e4",
        "outputId": "05df33f5-c84e-4382-df4d-ca7a34dc4628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
            "=== Function Output ===\n",
            "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate text informed by these passages, and reflect on its own generations using special tokens called reflection tokens. By incorporating retrieval and self-reflection, Self-RAG aims to improve the generation quality and factuality of the language model without compromising its original creativity and versatility.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
            "=== Function Output ===\n",
            "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to effectively fine-tune models to longer context lengths. This approach allows for significant context extension while retaining the original model architectures and compatibility with existing techniques. LongLoRA has shown strong empirical results on various tasks and models, demonstrating its effectiveness in efficiently extending context windows.\n",
            "=== LLM Response ===\n",
            "Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate text informed by these passages, and reflect on its own generations using special tokens called reflection tokens. By incorporating retrieval and self-reflection, Self-RAG aims to improve the generation quality and factuality of the language model without compromising its original creativity and versatility.\n",
            "\n",
            "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to effectively fine-tune models to longer context lengths. This approach allows for significant context extension while retaining the original model architectures and compatibility with existing techniques. LongLoRA has shown strong empirical results on various tasks and models, demonstrating its effectiveness in efficiently extending context windows.\n",
            "assistant: Self-RAG is a framework that enhances the quality and factuality of a large language model through retrieval and self-reflection. It involves training a single arbitrary language model to adaptively retrieve passages on-demand, generate text informed by these passages, and reflect on its own generations using special tokens called reflection tokens. By incorporating retrieval and self-reflection, Self-RAG aims to improve the generation quality and factuality of the language model without compromising its original creativity and versatility.\n",
            "\n",
            "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to effectively fine-tune models to longer context lengths. This approach allows for significant context extension while retaining the original model architectures and compatibility with existing techniques. LongLoRA has shown strong empirical results on various tasks and models, demonstrating its effectiveness in efficiently extending context windows.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7eede70c",
      "metadata": {
        "id": "7eede70c"
      },
      "source": [
        "## 2. Setup an agent over 11 papers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18771e69",
      "metadata": {
        "id": "18771e69"
      },
      "source": [
        "### Download 11 ICLR papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5",
      "metadata": {
        "height": 472,
        "tags": [],
        "id": "60d01d2c-547f-4054-b0fe-ed9b1a9cc3b5"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
        "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
        "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
        "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
        "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
        "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
        "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
        "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
        "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
        "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
        "    \"https://openreview.net/pdf?id=TpD2aG1h0D\"\n",
        "]\n",
        "\n",
        "papers = [\n",
        "    \"metagpt.pdf\",\n",
        "    \"longlora.pdf\",\n",
        "    \"loftq.pdf\",\n",
        "    \"swebench.pdf\",\n",
        "    \"selfrag.pdf\",\n",
        "    \"zipformer.pdf\",\n",
        "    \"values.pdf\",\n",
        "    \"finetune_fair_diffusion.pdf\",\n",
        "    \"knowledge_card.pdf\",\n",
        "    \"metra.pdf\",\n",
        "    \"vr_mcl.pdf\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b77426cb",
      "metadata": {
        "tags": [],
        "id": "b77426cb"
      },
      "source": [
        "To download these papers, below is the needed code:\n",
        "\n",
        "\n",
        "    #for url, paper in zip(urls, papers):\n",
        "         #!wget \"{url}\" -O \"{paper}\"\n",
        "    \n",
        "    \n",
        "**Note**: The pdf files are included with this lesson. To access these papers, go to the `File` menu and select`Open...`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
      "metadata": {
        "height": 166,
        "tags": [],
        "id": "ea5ee34d-02ac-4537-ae20-7ef6c5767172",
        "outputId": "c97a3e9e-0317-4eb2-9c83-8895d7b4a8de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Getting tools for paper: metagpt.pdf\n",
            "Getting tools for paper: longlora.pdf\n",
            "Getting tools for paper: loftq.pdf\n",
            "Getting tools for paper: swebench.pdf\n",
            "Getting tools for paper: selfrag.pdf\n",
            "Getting tools for paper: zipformer.pdf\n",
            "Getting tools for paper: values.pdf\n",
            "Getting tools for paper: finetune_fair_diffusion.pdf\n",
            "Getting tools for paper: knowledge_card.pdf\n",
            "Getting tools for paper: metra.pdf\n",
            "Getting tools for paper: vr_mcl.pdf\n"
          ]
        }
      ],
      "source": [
        "from utils import get_doc_tools\n",
        "from pathlib import Path\n",
        "\n",
        "paper_to_tools_dict = {}\n",
        "for paper in papers:\n",
        "    print(f\"Getting tools for paper: {paper}\")\n",
        "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
        "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e35d52c",
      "metadata": {
        "id": "4e35d52c"
      },
      "source": [
        "### Extend the Agent with Tool Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20154923-873e-4941-9a3a-4926ab5f9b8c",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "20154923-873e-4941-9a3a-4926ab5f9b8c"
      },
      "outputs": [],
      "source": [
        "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671582f9-70d7-4a8f-b813-58b2a068ca72",
      "metadata": {
        "height": 166,
        "tags": [],
        "id": "671582f9-70d7-4a8f-b813-58b2a068ca72"
      },
      "outputs": [],
      "source": [
        "# define an \"object\" index and retriever over these tools\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core.objects import ObjectIndex\n",
        "\n",
        "obj_index = ObjectIndex.from_objects(\n",
        "    all_tools,\n",
        "    index_cls=VectorStoreIndex,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3929882-e9dc-46ca-b495-53e3ed60340e",
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "c3929882-e9dc-46ca-b495-53e3ed60340e"
      },
      "outputs": [],
      "source": [
        "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03",
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "ba9cfecd-fe14-4da8-b9ba-b3d485d98a03"
      },
      "outputs": [],
      "source": [
        "tools = obj_retriever.retrieve(\n",
        "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "c270ffbf-69c7-48ea-a028-9ba25221cde5",
        "outputId": "e535a4fa-a4b5-4bff-b08f-7e7a89c22382"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ToolMetadata(description='Useful for summarization questions related to swebench', name='summary_tool_swebench', fn_schema=<class 'llama_index.core.tools.types.DefaultToolFnSchema'>, return_direct=False)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tools[2].metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7",
      "metadata": {
        "height": 268,
        "tags": [],
        "id": "9cc0a0b6-9858-4348-9ae0-1cd4160f3fb7"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker\n",
        "from llama_index.core.agent import AgentRunner\n",
        "\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    tool_retriever=obj_retriever,\n",
        "    llm=llm,\n",
        "    system_prompt=\"\"\" \\\n",
        "You are an agent designed to answer queries over a set of given papers.\n",
        "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
        "\n",
        "\"\"\",\n",
        "    verbose=True\n",
        ")\n",
        "agent = AgentRunner(agent_worker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
      "metadata": {
        "height": 115,
        "tags": [],
        "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
        "outputId": "4e0e375d-6752-4057-8ccb-16fbeea3266c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
            "=== Function Output ===\n",
            "The evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub issues and pull requests across popular Python repositories. It includes task instructions, issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. The dataset is designed to be challenging and realistic for evaluating language models in software engineering tasks. Task instances are validated through execution-based verification, ensuring that solutions and tests can be applied, installed, and run successfully on codebases. The dataset is continuously updated with new task instances based on merged pull requests from public repositories with permissive licenses. Models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama are evaluated on this dataset using BM25 and \"oracle\" retrieval settings to assess their ability to resolve issues and generate patch files.\n",
            "=== LLM Response ===\n",
            "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. \n",
            "\n",
            "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub issues and pull requests across popular Python repositories. It includes task instructions, issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. The dataset is designed to be challenging and realistic for evaluating language models in software engineering tasks. Task instances are validated through execution-based verification, ensuring that solutions and tests can be applied, installed, and run successfully on codebases. The dataset is continuously updated with new task instances based on merged pull requests from public repositories with permissive licenses. Models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama are evaluated on this dataset using BM25 and \"oracle\" retrieval settings to assess their ability to resolve issues and generate patch files.\n",
            "assistant: The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. \n",
            "\n",
            "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub issues and pull requests across popular Python repositories. It includes task instructions, issue text, retrieved files and documentation, example patch files, and prompts for generating patch files. The dataset is designed to be challenging and realistic for evaluating language models in software engineering tasks. Task instances are validated through execution-based verification, ensuring that solutions and tests can be applied, installed, and run successfully on codebases. The dataset is continuously updated with new task instances based on merged pull requests from public repositories with permissive licenses. Models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama are evaluated on this dataset using BM25 and \"oracle\" retrieval settings to assess their ability to resolve issues and generate patch files.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Tell me about the evaluation dataset used \"\n",
        "    \"in MetaGPT and compare it against SWE-Bench\"\n",
        ")\n",
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
        "outputId": "f0d045c7-1cd5-41ec-95e2-1518fdcb37cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_longlora with args: {\"input\": \"Approach in LongLoRA\"}\n",
            "=== Function Output ===\n",
            "The approach in LongLoRA involves efficiently extending the context length of large language models (LLMs) to significantly larger sizes while saving on computational costs compared to traditional full fine-tuning approaches. It utilizes shifted sparse attention (S2-Attn) during fine-tuning to approximate long context training effectively by splitting the context length into groups and conducting attention within each group individually. LongLoRA retains the original attention architecture during inference, making it compatible with existing optimization techniques and infrastructure. Additionally, LongLoRA bridges the gap between LoRA and full fine-tuning through trainable normalization and embedding layers, enabling the extension of LLMs to longer context lengths with minimal accuracy compromise.\n",
            "=== Calling Function ===\n",
            "Calling function: summary_tool_loftq with args: {\"input\": \"Approach in LoftQ\"}\n",
            "=== Function Output ===\n",
            "The approach in LoftQ integrates quantization and low-rank approximation techniques to approximate the original high-precision pre-trained weights. This method involves quantizing weight matrices using specified functions, obtaining low-rank approximations through Singular Value Decomposition (SVD), and applying an alternating optimization process to fine-tune the low-rank adapters. By freezing the integer weight matrix during fine-tuning and optimizing the low-rank adapters efficiently, LoftQ aims to reduce the memory footprint and computational costs of large language models while maintaining performance levels close to full finetuning.\n",
            "=== LLM Response ===\n",
            "The approach in LongLoRA focuses on extending the context length of large language models (LLMs) efficiently to significantly larger sizes while saving on computational costs compared to traditional full fine-tuning approaches. It utilizes shifted sparse attention (S2-Attn) during fine-tuning to approximate long context training effectively by splitting the context length into groups and conducting attention within each group individually. LongLoRA retains the original attention architecture during inference, making it compatible with existing optimization techniques and infrastructure. It also bridges the gap between LoRA and full fine-tuning through trainable normalization and embedding layers, enabling the extension of LLMs to longer context lengths with minimal accuracy compromise.\n",
            "\n",
            "On the other hand, LoftQ's approach integrates quantization and low-rank approximation techniques to approximate the original high-precision pre-trained weights. This method involves quantizing weight matrices using specified functions, obtaining low-rank approximations through Singular Value Decomposition (SVD), and applying an alternating optimization process to fine-tune the low-rank adapters. By freezing the integer weight matrix during fine-tuning and optimizing the low-rank adapters efficiently, LoftQ aims to reduce the memory footprint and computational costs of large language models while maintaining performance levels close to full fine-tuning.\n"
          ]
        }
      ],
      "source": [
        "response = agent.query(\n",
        "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
        "    \"Analyze the approach in each paper first. \"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}